{"paragraphs":[{"text":"%md\n# Curso Diplodatos Programación Distribuida Sobre Grandes Volumenes de Datos\n# Práctico parte 1\n","dateUpdated":"2018-10-22T06:54:49-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Curso Diplodatos Programación Distribuida Sobre Grandes Volumenes de Datos</h1>\n<h1>Práctico parte 1</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1540202089786_-1289568068","id":"20181017-122345_269738593","dateCreated":"2018-10-22T06:54:49-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:557"},{"text":"%md\n## Para hacer con RDD's","dateUpdated":"2018-10-22T06:54:49-0300","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Para hacer con RDD&rsquo;s</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1540202089787_-1289952817","id":"20181017-115740_1467308851","dateCreated":"2018-10-22T06:54:49-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:558"},{"title":"Ejercicio 1","text":"%md\n### Ejercicio 1\n\nCada línea del archivo `~/diplodatos_bigdata/ds/links_raw.txt` contiene un url de una página web seguido de los links que posee a otras páginas web:\n```\n<url> <url link 1> <url link 2> ... <url link n>\n```\n\nBasándose en la utilización de la técnica de *MapReduce* que se mostró en el programa `word count` del notebook **Clase 01 - Introducción a Spark** haga un programa en Spark que cuente la cantidad de links que apuntan a cada página.\n\n#### Ayuda\n\nA continuación está el comienzo del programa. Falta hacer el *MapReduce* y mostrar el resultado.\n","dateUpdated":"2018-10-22T06:54:49-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Ejercicio 1</h3>\n<p>Cada línea del archivo <code>~/diplodatos_bigdata/ds/links_raw.txt</code> contiene un url de una página web seguido de los links que posee a otras páginas web:</p>\n<pre><code>&lt;url&gt; &lt;url link 1&gt; &lt;url link 2&gt; ... &lt;url link n&gt;\n</code></pre>\n<p>Basándose en la utilización de la técnica de <em>MapReduce</em> que se mostró en el programa <code>word count</code> del notebook <strong>Clase 01 - Introducción a Spark</strong> haga un programa en Spark que cuente la cantidad de links que apuntan a cada página.</p>\n<h4>Ayuda</h4>\n<p>A continuación está el comienzo del programa. Falta hacer el <em>MapReduce</em> y mostrar el resultado.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1540202089787_-1289952817","id":"20181017-115555_1095258694","dateCreated":"2018-10-22T06:54:49-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:559"},{"text":"val baseDir = \"/users/fpalacios/bigdata/diplodatos_bigdata\" // llenar con el directorio git\n\nval lines = sc.textFile(baseDir + \"/ds/links_raw.txt\")\n\nval linksTo = lines\n                .flatMap(l => l.split(\" \").tail) // separo los urls y tomo solo los apuntados\n// Ahora linksTo tiene las paginas apuntadas\n\n// Completar los ...\n\nval invLinkCount = linksTo.map(w => (w,1))\n                    .reduceByKey((nx,ny) => nx+ny)\n\nval result = invLinkCount.sortBy(p => p._2, ascending = false)\n\nresult.collect() // traigo resultados\n      .take(10)\n      .foreach(p => println(p))\n","user":"anonymous","dateUpdated":"2018-10-22T07:17:28-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"baseDir: String = /users/fpalacios/bigdata/diplodatos_bigdata\nlines: org.apache.spark.rdd.RDD[String] = /users/fpalacios/bigdata/diplodatos_bigdata/ds/links_raw.txt MapPartitionsRDD[34] at textFile at <console>:30\nlinksTo: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[35] at flatMap at <console>:33\ninvLinkCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[37] at reduceByKey at <console>:38\nresult: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[42] at sortBy at <console>:36\n(http://www.yahoo.com/,199)\n(http://www.ca.gov/,169)\n(http://www.leginfo.ca.gov/calaw.html,155)\n(http://www.linkexchange.com/,134)\n(http://www.berkeley.edu/,126)\n(http://www.sen.ca.gov/,123)\n(http://home.netscape.com/comprod/mirror/index.html,109)\n(http://www.assembly.ca.gov/,99)\n(http://www.epa.gov/,95)\n(http://www.usgs.gov/,84)\n"}]},"apps":[],"jobName":"paragraph_1540202089787_-1289952817","id":"20181017-115831_2142696179","dateCreated":"2018-10-22T06:54:49-0300","dateStarted":"2018-10-22T07:17:28-0300","dateFinished":"2018-10-22T07:17:30-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:560"},{"text":"%md\n\n### Ejercicio 2\n\nComplete los `...` en el siguiente programa para contar la cantidad de veces que aparece la letra 'c' en los archivos en `./logs/`.\n\n#### Ayuda\n\n\n* Se puede usar el método `.filter` (ya visto en ejemplos anteriores) para crear un RDD solo con la letra C.\n* El método `count` de RDD cuenta la cantidad de elementos.\n* La letra 'c' se escribe `'c'` en Scala.\n","dateUpdated":"2018-10-22T06:54:49-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Ejercicio 2</h3>\n<p>Complete los <code>...</code> en el siguiente programa para contar la cantidad de veces que aparece la letra &lsquo;c&rsquo; en los archivos en <code>./logs/</code>.</p>\n<h4>Ayuda</h4>\n<ul>\n  <li>Se puede usar el método <code>.filter</code> (ya visto en ejemplos anteriores) para crear un RDD solo con la letra C.</li>\n  <li>El método <code>count</code> de RDD cuenta la cantidad de elementos.</li>\n  <li>La letra &lsquo;c&rsquo; se escribe <code>&#39;c&#39;</code> en Scala.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1540202089788_-1291876561","id":"20181017-122021_611447568","dateCreated":"2018-10-22T06:54:49-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:561"},{"text":"val linesRDD = sc.textFile(\"/users/fpalacios/bigdata/diplodatos_bigdata/spark/zeppelin-0.7.3-bin-all/logs/\")\n\nval charsRDD = linesRDD\n                .flatMap(l => l)\n\nval onlyCRDD = charsRDD.\n                    filter(c => c == 'c')\n                    \n\nonlyCRDD.count\n","user":"anonymous","dateUpdated":"2018-10-22T07:58:30-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"linesRDD: org.apache.spark.rdd.RDD[String] = /users/fpalacios/bigdata/diplodatos_bigdata/spark/zeppelin-0.7.3-bin-all/logs/ MapPartitionsRDD[97] at textFile at <console>:27\ncharsRDD: org.apache.spark.rdd.RDD[Char] = MapPartitionsRDD[98] at flatMap at <console>:31\nonlyCRDD: org.apache.spark.rdd.RDD[Char] = MapPartitionsRDD[99] at filter at <console>:33\nres125: Long = 68456\n"}]},"apps":[],"jobName":"paragraph_1540202089788_-1291876561","id":"20181017-122027_1935632241","dateCreated":"2018-10-22T06:54:49-0300","dateStarted":"2018-10-22T07:58:30-0300","dateFinished":"2018-10-22T07:58:30-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:562"},{"text":"%md\n### Ejercicio 3\n\nModifique el siguiente programa para que cuente la cantidad de lineas que comienzan con la palabra `INFO`, `WARN` y `ERROR`.\n\nHaga cache de los RDD para hacer el programa más eficiente. \n","dateUpdated":"2018-10-22T06:54:49-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Ejercicio 3</h3>\n<p>Modifique el siguiente programa para que cuente la cantidad de lineas que comienzan con la palabra <code>INFO</code>, <code>WARN</code> y <code>ERROR</code>.</p>\n<p>Haga cache de los RDD para hacer el programa más eficiente.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1540202089788_-1291876561","id":"20181017-122317_440108867","dateCreated":"2018-10-22T06:54:49-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:563"},{"text":"val linesRDD = sc.textFile(\"/users/fpalacios/bigdata/diplodatos_bigdata/spark/zeppelin-0.7.3-bin-all/logs/\") // RDD de entrada\n\nval linesTrim = linesRDD.map(l => l.trim) // Borro espacios en borde\n\nval linesInfo = linesTrim.filter(l => l.startsWith(\"INFO\"))\n\nval linesWarn = linesTrim.filter(l => l.startsWith(\"WARN\"))\n\nval linesError = linesTrim.filter(l => l.startsWith(\"ERROR\"))\n\nprintln(\"Cantidad de lineas INFO: \" + linesInfo.count )\n\nprintln(\"Cantidad de lineas WARN: \" + linesWarn.count  ) //Completar\n\nprintln(\"Cantidad de lineas ERROR: \" + linesError.count  )  //Completar\n\n","user":"anonymous","dateUpdated":"2018-10-23T07:00:24-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"linesRDD: org.apache.spark.rdd.RDD[String] = /users/fpalacios/bigdata/diplodatos_bigdata/spark/zeppelin-0.7.3-bin-all/logs/ MapPartitionsRDD[9] at textFile at <console>:27\nlinesTrim: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[10] at map at <console>:30\nlinesInfo: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[11] at filter at <console>:32\nlinesWarn: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[12] at filter at <console>:32\nlinesError: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[13] at filter at <console>:32\nCantidad de lineas INFO: 18221\nCantidad de lineas WARN: 245\nCantidad de lineas ERROR: 32\n"}]},"apps":[],"jobName":"paragraph_1540202089789_-1292261310","id":"20181017-122341_2016787069","dateCreated":"2018-10-22T06:54:49-0300","dateStarted":"2018-10-23T07:00:24-0300","dateFinished":"2018-10-23T07:00:27-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:564"},{"text":"%md\n### Ejercicio 4\n\nEl archivo en `~/diplodatos_bigdata/ds/flights.csv` contiene información de vuelos realizados en 2008 (solo 100.000), uno por línea.\n\nLos datos estan separados por coma y la columna 22 tiene un `1` si el vuelo fue cancelado. Además si el vuelo fue redirigido se indica con '1' en la columna 24.\n\nCompletar el siguiente programa que devuelve el porcentaje de vuelos cancelados y el porcentaje de redirigidos.\n\nUtilizar cache si lo cree conveniente.\n","dateUpdated":"2018-10-22T06:54:49-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Ejercicio 4</h3>\n<p>El archivo en <code>~/diplodatos_bigdata/ds/flights.csv</code> contiene información de vuelos realizados en 2008 (solo 100.000), uno por línea.</p>\n<p>Los datos estan separados por coma y la columna 22 tiene un <code>1</code> si el vuelo fue cancelado. Además si el vuelo fue redirigido se indica con &lsquo;1&rsquo; en la columna 24.</p>\n<p>Completar el siguiente programa que devuelve el porcentaje de vuelos cancelados y el porcentaje de redirigidos.</p>\n<p>Utilizar cache si lo cree conveniente.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1540202089789_-1292261310","id":"20181017-123340_1748479084","dateCreated":"2018-10-22T06:54:49-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:565"},{"text":"val input = sc.textFile(\"/users/fpalacios/bigdata/diplodatos_bigdata/ds/flights.csv\") // Completar el path\n\nval nTotal = input.count - 1 // la primer fila tiene el nombre de las columnas\n\nval parsed = input.map(l => l.split(\",\"))\n\nval cancel = parsed.filter(l => l(21) == \"1\") // Completar\n\nval redir =  parsed.filter(l => l(23) == \"1\") //completar\n\nval nCancel = cancel.count\nval nRedir = redir.count\n\nprintln(\"% cancelados = \" + nCancel.toDouble * 100 / nTotal)\nprintln(\"% redireccionados = \" + nRedir.toDouble * 100 / nTotal ) // Completar\n","user":"anonymous","dateUpdated":"2018-10-24T06:49:00-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"input: org.apache.spark.rdd.RDD[String] = /users/fpalacios/bigdata/diplodatos_bigdata/ds/flights.csv MapPartitionsRDD[7] at textFile at <console>:27\nnTotal: Long = 100000\nparsed: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[8] at map at <console>:30\ncancel: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[9] at filter at <console>:32\nredir: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[10] at filter at <console>:32\nnCancel: Long = 1142\nnRedir: Long = 160\n% cancelados = 1.142\n% redireccionados = 0.16\n"}]},"apps":[],"jobName":"paragraph_1540202089789_-1292261310","id":"20181017-123355_1061822713","dateCreated":"2018-10-22T06:54:49-0300","dateStarted":"2018-10-24T06:49:00-0300","dateFinished":"2018-10-24T06:49:02-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:566"},{"text":"%md\n### Ejercicio 5\n\nLa columna 14 del mismo archivo tiene el tiempo del vuelo en minutos. Calcular el máximo.\n\n#### Ayuda\n\n* Busque en la documentacion de la [API RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) una acción para calcular el máximo.\n* Ojo que puede haber valores no definidos.\n","dateUpdated":"2018-10-22T06:54:49-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Ejercicio 5</h3>\n<p>La columna 14 del mismo archivo tiene el tiempo del vuelo en minutos. Calcular el máximo.</p>\n<h4>Ayuda</h4>\n<ul>\n  <li>Busque en la documentacion de la <a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD\">API RDD</a> una acción para calcular el máximo.</li>\n  <li>Ojo que puede haber valores no definidos.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1540202089790_-1291107063","id":"20181017-123421_149337337","dateCreated":"2018-10-22T06:54:49-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:567"},{"text":"// Acá hay que escribir el programa\n\nval input = sc.textFile(\"/users/fpalacios/bigdata/diplodatos_bigdata/ds/flights.csv\") // Completar el path\n\n\nval parsed = input.map(l => l.split(\",\"))\n\nval tiempo = parsed.filter( l => l(13).forall( c => c.isDigit ))\n                   .filter( l => l(13) != \"\" )\n                   .map( l => l(13).toInt ) \n\nprintln(\"El máximo es \" + tiempo.max() )\n","user":"anonymous","dateUpdated":"2018-10-24T06:48:19-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"input: org.apache.spark.rdd.RDD[String] = /users/fpalacios/bigdata/diplodatos_bigdata/ds/flights.csv MapPartitionsRDD[1] at textFile at <console>:29\nparsed: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at <console>:31\ntiempo: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at map at <console>:34\nEl máximo es 369\n"}]},"apps":[],"jobName":"paragraph_1540202089790_-1291107063","id":"20181017-123612_1693858747","dateCreated":"2018-10-22T06:54:49-0300","dateStarted":"2018-10-24T06:48:19-0300","dateFinished":"2018-10-24T06:48:34-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:568"},{"text":"%md\n\n## Para hacer con **Spark SQL**","dateUpdated":"2018-10-22T06:54:49-0300","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Para hacer con <strong>Spark SQL</strong></h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1540202089790_-1291107063","id":"20181017-123455_735765736","dateCreated":"2018-10-22T06:54:49-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:569"},{"title":"Ejercicio 6","text":"%md\n\n### Ejercicio 6\n\nComplete el código siguiente para calcular en un Dataframe la cantidad de usuarios por pais desagregando por sexo usando SQL plano o programático.\n","dateUpdated":"2018-10-22T06:54:49-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Ejercicio 6</h3>\n<p>Complete el código siguiente para calcular en un Dataframe la cantidad de usuarios por pais desagregando por sexo usando SQL plano o programático.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1540202089790_-1291107063","id":"20181017-123555_742276598","dateCreated":"2018-10-22T06:54:49-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:570"},{"text":"// Lectura de csv\nval profiles = spark.read.format(\"csv\")\n                .option(\"delimiter\", \"\\\\t\")\n                .option(\"header\", \"true\")\n                .option(\"inferSchema\", \"true\")\n                .load(\"/users/fpalacios/bigdata/diplodatos_bigdata/ds/userid-profile.tsv\")\n\n// Creación de tabla temporal\nprofiles.createOrReplaceTempView(\"users\")\n    \n// Con SQL plano\nval nUsr4CtryGen = spark.sql(\"SELECT country, gender, count(*) AS cantidad FROM users GROUP BY country, gender ORDER BY country, gender\")\n\nnUsr4CtryGen.show\n\n","user":"anonymous","dateUpdated":"2018-10-24T07:49:13-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"profiles: org.apache.spark.sql.DataFrame = [id: string, gender: string ... 3 more fields]\nnUsr4CtryGen: org.apache.spark.sql.DataFrame = [country: string, gender: string ... 1 more field]\n+--------------------+------+--------+\n|             country|gender|cantidad|\n+--------------------+------+--------+\n|                null|  null|      49|\n|                null|     f|      23|\n|                null|     m|      13|\n|             Algeria|     m|       1|\n|          Antarctica|     m|       1|\n|           Argentina|  null|       1|\n|           Argentina|     f|       3|\n|           Argentina|     m|       5|\n|             Armenia|     f|       1|\n|           Australia|     f|      10|\n|           Australia|     m|      12|\n|             Austria|     f|       1|\n|             Austria|     m|       2|\n|             Belarus|     m|       1|\n|             Belgium|  null|       2|\n|             Belgium|     f|       1|\n|             Belgium|     m|       6|\n|Bosnia and Herzeg...|     f|       1|\n|              Brazil|     f|       8|\n|              Brazil|     m|      12|\n+--------------------+------+--------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1540202089791_-1291491812","id":"20181017-165609_830357654","dateCreated":"2018-10-22T06:54:49-0300","dateStarted":"2018-10-24T07:49:13-0300","dateFinished":"2018-10-24T07:49:14-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:571"},{"text":"// Con SQL porgramático\n\nval nUsr4CtryGen2 = profiles\n                .groupBy($\"country\",$\"gender\").agg(count($\"*\").as(\"cantidad\"))\n                .orderBy($\"country\",$\"gender\")\n\nnUsr4CtryGen2.show\n","user":"anonymous","dateUpdated":"2018-10-24T07:12:13-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"nUsr4CtryGen2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [country: string, gender: string ... 1 more field]\n+--------------------+------+--------+\n|             country|gender|cantidad|\n+--------------------+------+--------+\n|                null|  null|      49|\n|                null|     f|      23|\n|                null|     m|      13|\n|             Algeria|     m|       1|\n|          Antarctica|     m|       1|\n|           Argentina|  null|       1|\n|           Argentina|     f|       3|\n|           Argentina|     m|       5|\n|             Armenia|     f|       1|\n|           Australia|     f|      10|\n|           Australia|     m|      12|\n|             Austria|     f|       1|\n|             Austria|     m|       2|\n|             Belarus|     m|       1|\n|             Belgium|  null|       2|\n|             Belgium|     f|       1|\n|             Belgium|     m|       6|\n|Bosnia and Herzeg...|     f|       1|\n|              Brazil|     f|       8|\n|              Brazil|     m|      12|\n+--------------------+------+--------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1540202089791_-1291491812","id":"20181017-165655_447564231","dateCreated":"2018-10-22T06:54:49-0300","dateStarted":"2018-10-24T07:12:13-0300","dateFinished":"2018-10-24T07:12:14-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:572"},{"text":"%md\n\n### Ejercicio 7\nComplete el siguiente programa par calcular la edad promedio por género y guarde el resultado como tabla SQL y como archivo parquet.\n","dateUpdated":"2018-10-22T06:54:49-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Ejercicio 7</h3>\n<p>Complete el siguiente programa par calcular la edad promedio por género y guarde el resultado como tabla SQL y como archivo parquet.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1540202089791_-1291491812","id":"20181017-165936_1307027043","dateCreated":"2018-10-22T06:54:49-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:573"},{"text":"// Como tabla SQL\n\nspark.sql(\"drop table if exists gen_prom\") // borro la tabla si existe\n\n//val result_users = spark.sql(\"SELECT gender, avg(age) AS age_avg FROM users GROUP BY gender\")\n\n//result_users.show\n\n\nspark.sql(\"create table gen_prom as SELECT gender, avg(age) AS age_avg FROM users GROUP BY gender\")\n\n// Cargo tabla y muestro su contenido\nspark.sql(\"select * from gen_prom\").show\n","user":"anonymous","dateUpdated":"2018-10-24T07:29:03-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res30: org.apache.spark.sql.DataFrame = []\nres37: org.apache.spark.sql.DataFrame = []\n+------+------------------+\n|gender|           age_avg|\n+------+------------------+\n|     m|25.630573248407643|\n|     f| 24.13157894736842|\n|  null|              32.0|\n+------+------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1540202089791_-1291491812","id":"20181017-165956_242290252","dateCreated":"2018-10-22T06:54:49-0300","dateStarted":"2018-10-24T07:29:03-0300","dateFinished":"2018-10-24T07:29:09-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:574"},{"text":"// Como parquet\n\nval genProm = profiles\n                .groupBy($\"gender\").agg(avg($\"age\").as(\"age_avg\"))\n\nimport org.apache.spark.sql.SaveMode\n\ngenProm.write.mode(SaveMode.Overwrite).save(\"./gen_prom.parquet\")\n\n// Cargo parquet y muestro su contenido\nspark.read.load(\"./gen_prom.parquet\").show\n","dateUpdated":"2018-10-24T07:32:24-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1540202089792_-1305727522","id":"20181017-170034_97474385","dateCreated":"2018-10-22T06:54:49-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:575","user":"anonymous","dateFinished":"2018-10-24T07:32:26-0300","dateStarted":"2018-10-24T07:32:24-0300","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"genProm: org.apache.spark.sql.DataFrame = [gender: string, age_avg: double]\nimport org.apache.spark.sql.SaveMode\n+------+------------------+\n|gender|           age_avg|\n+------+------------------+\n|     f| 24.13157894736842|\n|     m|25.630573248407643|\n|  null|              32.0|\n+------+------------------+\n\n"}]}},{"text":"%md\n\n### Ejercicio 8\nCon el dataset `profiles` complete el siguiente código para calcular la cantidad de registraciones por día de la semana.\n\n#### Ayuda:\n\nEn [SQL API Function Reference](http://spark.apache.org/docs/2.1.1/api/scala/index.html#org.apache.spark.sql.functions$) en la sección \"Date time functions\" hay métodos para manipular fechas.\n","dateUpdated":"2018-10-22T06:54:49-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Ejercicio 8</h3>\n<p>Con el dataset <code>profiles</code> complete el siguiente código para calcular la cantidad de registraciones por día de la semana.</p>\n<h4>Ayuda:</h4>\n<p>En <a href=\"http://spark.apache.org/docs/2.1.1/api/scala/index.html#org.apache.spark.sql.functions$\">SQL API Function Reference</a> en la sección &ldquo;Date time functions&rdquo; hay métodos para manipular fechas.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1540202089792_-1305727522","id":"20181017-170058_1102792330","dateCreated":"2018-10-22T06:54:49-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:576"},{"text":"val regByDayOfWeek = profiles.select($\"registered\", unix_timestamp($\"registered\",\"MMM dd, yyyy\").as(\"reg_sec\"))\n                   .select($\"*\", from_unixtime($\"reg_sec\",\"E\").as(\"day_week\"))\n\n\nz.show(regByDayOfWeek.groupBy($\"day_week\").count)","dateUpdated":"2018-10-24T07:50:11-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1540202089792_-1305727522","id":"20181017-170140_958150946","dateCreated":"2018-10-22T06:54:49-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:577","user":"anonymous","dateFinished":"2018-10-24T07:50:11-0300","dateStarted":"2018-10-24T07:50:11-0300","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"regByDayOfWeek: org.apache.spark.sql.DataFrame = [registered: string, reg_sec: bigint ... 1 more field]\n"},{"type":"TABLE","data":"day_week\tcount\nSun\t148\nnull\t8\nMon\t147\nThu\t131\nSat\t125\nWed\t155\nFri\t131\nTue\t147\n"}]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1540377425231_-1545807900","id":"20181024-073705_1544143681","dateCreated":"2018-10-24T07:37:05-0300","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2085"}],"name":"Práctico (parte 1)","id":"2DTCSFP1C","angularObjects":{"2CVPMZQDC:shared_process":[],"2CVZRWYXD:shared_process":[],"2CXC3J873:shared_process":[],"2CW9KVZFB:shared_process":[],"2CWBZB5J2:shared_process":[],"2CWQPZNH6:shared_process":[],"2CWB7GZBS:shared_process":[],"2CUMUUMCY:shared_process":[],"2CX36SA9F:shared_process":[],"2CVWTU38R:shared_process":[],"2CVGJFNZ4:shared_process":[],"2CUKDKYCE:shared_process":[],"2CWDFCUCZ:shared_process":[],"2CUNFHEUZ:shared_process":[],"2CW49AZMR:shared_process":[],"2CWE35VRJ:shared_process":[],"2CWV6WPV3:shared_process":[],"2CXGQWA1W:shared_process":[],"2CUU3PNNG:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}